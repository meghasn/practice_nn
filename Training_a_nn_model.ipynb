{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training a nn model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "a logarithm is in the format\n",
        "\n",
        "e**x=b"
      ],
      "metadata": {
        "id": "BIFVp_30i-OJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuYYUJ6Giufz",
        "outputId": "7b8426e6-98a7-4d2a-b6f0-ebf9711d7982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7578579175523736\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "b=5.8\n",
        "print(np.log(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to get crossentropy - we take the negative sum of (target value * log(predicted value))"
      ],
      "metadata": {
        "id": "ZSv0jE13jhNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "softmax_output=[.7,.1,.2]\n",
        "target_op=[1,0,0]\n",
        "target_class=[0]\n",
        "loss=-(math.log(softmax_output[0])*target_op[0]+math.log(softmax_output[1])*target_op[1]+math.log(softmax_output[2])*target_op[2])\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B08oBfD0jyuL",
        "outputId": "f08846e3-28e5-4a3e-9a91-0fdcf28d6011"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "softmax_output=np.array([[.7,.1,.2],[.1,.5,.4],[.02,.9,.08]])\n",
        "class_targets=[0,1,1]\n",
        "print(np.mean(-np.log(softmax_output[[0,1,2],class_targets])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoV-u4w0vCWU",
        "outputId": "d940eedb-f491-4516-8d15-072319329d85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YMV0i2OzFti",
        "outputId": "f24d4dc3-79a2-49e7-fc34-fcc1e7dcb214"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "X,y=spiral_data(100,4)\n",
        "class activation_relu:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights=.1*np.random.randn(n_inputs,n_neurons)#randn is just a gaussian distribution around 0, if values are more than  1 multiply by .1\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights)+self.biases\n",
        "class activation_softmax:\n",
        "  def forward(self,inputs):\n",
        "    exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probabilities=exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output=probabilities\n",
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "    sample_loss=self.forward(output,y)\n",
        "    data_loss=np.mean(sample_loss)\n",
        "    return data_loss\n",
        "class categorical_crossentropy(Loss):\n",
        "  def forward(self,ypred,ytrue):\n",
        "    samples=len(ypred)\n",
        "    ypred_clipped=np.clip(ypred,1e-7,1-1e-7)#to prevent the infinity issue\n",
        "    #we should be able to handle both scalar and one hot encoded values\n",
        "    if len(ytrue.shape)==1:#scalar\n",
        "      print(ytrue.shape,range(samples))\n",
        "      correct_confidence=ypred_clipped[range(samples),ytrue]\n",
        "    elif len(ytrue.shape)==2:\n",
        "      correct_confidence=np.sum(ypred_clipped*ytrue,axis=1)\n",
        "    negative_log_likelihood=-np.log(correct_confidence)\n",
        "    return negative_log_likelihood #vector of values\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "define the layers\n",
        "\"\"\"\n",
        "dense1=Layer_Dense(2,3)\n",
        "activation1=activation_relu()\n",
        "dense2=Layer_Dense(3,4)#ip is the op of prev layer\n",
        "activation2=activation_softmax()\n",
        "\"\"\"\n",
        "start propagation\n",
        "\"\"\"\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "# print(activation2.output)\n",
        "\n",
        "loss_fn=categorical_crossentropy()\n",
        "loss=loss_fn.calculate(activation2.output,y)\n",
        "\n",
        "print(\"loss\",loss)"
      ],
      "metadata": {
        "id": "NuTbByhwzIgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dcbc9e-6c2f-42b9-c32a-3e47f393008a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400,) range(0, 400)\n",
            "loss 1.3869831630086862\n"
          ]
        }
      ]
    }
  ]
}